{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "680a4507",
   "metadata": {},
   "source": [
    "# PA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a93c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "081e3c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"data\"\n",
    "docs = []\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(root, file_name)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            doc = file.read()\n",
    "            docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4ca4f",
   "metadata": {},
   "source": [
    "## q1\n",
    "Construct a dictionary based on the terms extracted from the given documents.\n",
    "- Record the document frequency of each term.\n",
    "- Save your dictionary as a txt file (dictionary.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e84439d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3293992\n"
     ]
    }
   ],
   "source": [
    "docs_txt = \" \".join(docs)\n",
    "print(len(docs_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e77b26",
   "metadata": {},
   "source": [
    "### tokenize (pa1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4269536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc: str, stopword: str):\n",
    "    # lowercase\n",
    "    doc = doc.lower()\n",
    "    # 去除特殊符號\n",
    "    # 以空格取代特殊符號\n",
    "    new_doc = \"\"\n",
    "    for i in doc:\n",
    "        if (i >= 'a' and i <= 'z') or (i == ' '):\n",
    "            new_doc += i\n",
    "        else:\n",
    "            new_doc += ' '\n",
    "\n",
    "    # 斷詞\n",
    "    tokens = [i for i in new_doc.split(\" \")]\n",
    "\n",
    "    # Stopword Removal\n",
    "    stop_tokens = [i for i in tokens if i not in stopword]\n",
    "\n",
    "    # Stemming\n",
    "    porter = PorterStemmer()\n",
    "    stemming_tokens = [porter.stem(i) for i in stop_tokens]\n",
    "\n",
    "    return stemming_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bd47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每份文件詞頻\n",
    "def calcu_freq(tokenized_words: list):\n",
    "    word_dict = {}\n",
    "    for i in tokenized_words:\n",
    "        if i in word_dict:\n",
    "            word_dict[i] += 1\n",
    "        else:\n",
    "            word_dict[i] = 1\n",
    "\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba045cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算每個 term 的 df\n",
    "def calcu_df(all_terms: list, all_tokenized_doc: list):\n",
    "    df_list = []\n",
    "    for term in all_terms:\n",
    "        df = 0\n",
    "        for temp_doc in all_tokenized_doc:\n",
    "            if term in temp_doc:\n",
    "                df += 1\n",
    "        df_list.append([term, df])\n",
    "    \n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03f97b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords.txt\", \"r\") as file:\n",
    "    stopword = file.read()\n",
    "\n",
    "# 每篇文章 tokenize\n",
    "# [[doc1], [doc2], ...]\n",
    "all_tokenized_doc = [tokenize(i, stopword) for i in docs]\n",
    "\n",
    "# 統計總共有哪些 term\n",
    "all_tokenized_words = [i for k in all_tokenized_doc for i in k]\n",
    "all_terms = list(set(all_tokenized_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced9ce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = calcu_df(all_terms, all_tokenized_doc)\n",
    "print(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de0214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 dict 存成 txt 並 sort\n",
    "df = pd.DataFrame(df_list, columns=['term', 'df'])\n",
    "# sort by term\n",
    "df_sorted = df.sort_values(by='term', ascending=True).reset_index(drop=True)\n",
    "# 製作 index\n",
    "df_sorted['t_index'] = df_sorted.index + 1\n",
    "# reorder columns\n",
    "df_final = df_sorted[['t_index', 'term', 'df']]\n",
    "\n",
    "df_final.to_csv(\"output/dictionary.txt\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bc41bb",
   "metadata": {},
   "source": [
    "## q2\n",
    "Transfer each document into a tf-idf unit vector\n",
    "- Save it as a txt file (DocID.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93172077",
   "metadata": {},
   "source": [
    "- tf = (該 term 在該文件出現次數) / (該文件 term 總數)\n",
    "- idf = log(N / df)\n",
    "  - N = 文件總數\n",
    "  - df = 包含該 term 的文件數\n",
    "- Unit Vector\n",
    "  - normalization\n",
    "  - tfidf_i / |V|\n",
    "  - |V| = sqrt(sum(tfidf_i ^ 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa99685a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095\n"
     ]
    }
   ],
   "source": [
    "term_df = pd.read_csv(\"output/dictionary.txt\")\n",
    "term_df = term_df.set_index(\"term\")\n",
    "\n",
    "with open(\"stopwords.txt\", \"r\") as file:\n",
    "    stopword = file.read()\n",
    "\n",
    "# 文件總數\n",
    "N = len(all_tokenized_doc)\n",
    "print(N)\n",
    "\n",
    "# traverse each document\n",
    "for idx, one_tokenized_doc in enumerate(all_tokenized_doc):\n",
    "    \n",
    "    # 計算該文件 term freq\n",
    "    word_dict = calcu_freq(one_tokenized_doc)\n",
    "    # 該文件長度\n",
    "    term_len = len(one_tokenized_doc)\n",
    "    \n",
    "    tfidf_list = []\n",
    "    visited = []\n",
    "    # traverse each term in a document\n",
    "    for term in one_tokenized_doc:\n",
    "        # 確保不要有重複的 term \n",
    "        if term in visited:\n",
    "            continue\n",
    "        visited.append(term)\n",
    "        # 計算 tf\n",
    "        tf = word_dict[term] / term_len\n",
    "\n",
    "        # 抓 dictionary 內的 df\n",
    "        row = term_df.loc[term]\n",
    "        t_index = row['t_index']\n",
    "        t_df = row['df']\n",
    "\n",
    "        idf = math.log10(N / t_df)\n",
    "\n",
    "        tfidf = tf * idf\n",
    "\n",
    "        \n",
    "        tfidf_list.append([t_index, tfidf])\n",
    "    \n",
    "    \n",
    "    tfidf_list = pd.DataFrame(tfidf_list, columns=['t_index', 'tf-idf'])\n",
    "    # Unit Vector normalization\n",
    "    tfidf_val = tfidf_list.copy()\n",
    "    tfidf_val['v_length'] = tfidf_val['tf-idf'] ** 2\n",
    "    v_length = math.sqrt(tfidf_val['v_length'].sum())\n",
    "    tfidf_list['tf-idf'] = round(tfidf_list['tf-idf'] / v_length, 3)\n",
    "\n",
    "\n",
    "    tfidf_list = tfidf_list.sort_values(by='t_index', ascending=True).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    save_path = \"output/\" + str(idx + 1) + \".txt\"\n",
    "    tfidf_list.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9b604",
   "metadata": {},
   "source": [
    "## q3\n",
    "Write a function cosine(Docx, Docy) which loads the tf-idf\n",
    "vectors of documents x and y and returns their cosine \n",
    "similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414e24fa",
   "metadata": {},
   "source": [
    "cosine similarity = (A · B) / (|A| * |B|)\n",
    "- A · B -> 交集向量相乘後加總\n",
    "- |A| -> sqrt(sum(tfidf ^ 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7e982bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(DocX, DocY):\n",
    "    DocX_df = pd.read_csv(DocX)\n",
    "    DocY_df = pd.read_csv(DocY)\n",
    "\n",
    "    # 找出交集的 term\n",
    "    merged_df = pd.merge(DocX_df, DocY_df, on='t_index', how='inner', suffixes=('_x', '_y'))\n",
    "    # 計算 X·Y\n",
    "    merged_df['dot'] = merged_df['tf-idf_x'] * merged_df['tf-idf_y']\n",
    "    inner_product = merged_df['dot'].sum()\n",
    "\n",
    "    # 計算 |X|\n",
    "    DocX_df['X_length'] = DocX_df['tf-idf'] ** 2\n",
    "    X_length = math.sqrt(DocX_df['X_length'].sum())\n",
    "    # 計算 |Y|\n",
    "    DocY_df['Y_length'] = DocY_df['tf-idf'] ** 2\n",
    "    Y_length = math.sqrt(DocY_df['Y_length'].sum())\n",
    "\n",
    "    cosine_similarity = inner_product / (X_length * Y_length)\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c4f24e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27714960226192065\n"
     ]
    }
   ],
   "source": [
    "DocX = \"output/1.txt\"\n",
    "DocY = \"output/2.txt\"\n",
    "cosine_similarity = cosine(DocX, DocY)\n",
    "print(cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a650b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
